import os
import json
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin

BASE_URL = "https://yevshan.com.ua"
MIN_YEAR = 2025  # —Ç—ñ–ª—å–∫–∏ –Ω–æ–≤–∏–Ω–∏ –ø—ñ—Å–ª—è —Ü—å–æ–≥–æ —Ä–æ–∫—É

def get_news_links(quan=10):
    """–û—Ç—Ä–∏–º—É—î —Å–ø–∏—Å–æ–∫ –Ω–æ–≤–∏–Ω —á–µ—Ä–µ–∑ POST-–∑–∞–ø–∏—Ç –¥–æ loadNews.php"""
    try:
        res = requests.post(
            urljoin(BASE_URL, "php/functions/loadNews.php"),
            data={"quan": quan}
        )
        # –í–∏–ø—Ä–∞–≤–ª–µ–Ω–Ω—è –Ω–µ–∫–æ—Ä–µ–∫—Ç–Ω–∏—Ö —Å–∏–º–≤–æ–ª—ñ–≤ —É JSON
        text = res.text.replace('\n', '').replace('\r', '')
        data = json.loads(text)
        # news_links –∑–±–∏—Ä–∞—î–º–æ –∑ data['left'] —Ç–∞ data['right'] —è–∫ HTML
        soup_left = BeautifulSoup(data.get("left", ""), "html.parser")
        soup_right = BeautifulSoup(data.get("right", ""), "html.parser")
        links = []
        for a_tag in soup_left.find_all("a") + soup_right.find_all("a"):
            href = a_tag.get("href")
            if href:
                full_url = urljoin(BASE_URL, href)
                links.append(full_url)
        return links
    except Exception as e:
        print("‚ùå –ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –æ—Ç—Ä–∏–º–∞–Ω–Ω—ñ –Ω–æ–≤–∏–Ω:", e)
        return []

def get_news_details(news_url):
    """–û—Ç—Ä–∏–º—É—î –¥–µ—Ç–∞–ª—ñ –Ω–æ–≤–∏–Ω–∏"""
    try:
        res = requests.get(news_url)
        if res.status_code != 200:
            print(f"–ü–æ–º–∏–ª–∫–∞ –¥–æ—Å—Ç—É–ø—É –¥–æ {news_url}")
            return None
        soup = BeautifulSoup(res.text, "html.parser")
        title_div = soup.find("div", class_="content-post-title")
        title = title_div.text.strip() if title_div else "–ë–µ–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞"

        updateDate_div = soup.find("div", class_="content-post-info-date")
        updateDate = updateDate_div.text.strip() if updateDate_div else "0000-00-00"

        # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ —Ä–æ–∫—É
        try:
            year = int(updateDate.split('.')[-1])
            if year < MIN_YEAR:
                return None
        except:
            pass

        fb_root_div = soup.find("div", id="fb-root")
        content = []
        current = title_div.find_next_sibling() if title_div else None
        while current and current != fb_root_div:
            content.append(str(current))
            current = current.find_next_sibling()
        full_content_html = "\n".join(content)

        return {"title": title, "full_text": full_content_html, "updateDate": updateDate, "url": news_url}
    except Exception as e:
        print(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥—É {news_url}: {e}")
        return None

def export_to_json(news_list, filename="news_after_2025.json"):
    with open(filename, "w", encoding="utf-8") as f:
        json.dump(news_list, f, ensure_ascii=False, indent=4)
    print(f"‚úÖ –ó–±–µ—Ä–µ–∂–µ–Ω–æ {len(news_list)} –Ω–æ–≤–∏–Ω —É {filename}")

def download_image(url, folder="images"):
    try:
        if not os.path.exists(folder):
            os.makedirs(folder)
        response = requests.get(url, stream=True)
        if response.status_code == 200:
            filename = os.path.join(folder, url.split("/")[-1])
            with open(filename, "wb") as f:
                for chunk in response.iter_content(1024):
                    f.write(chunk)
            print(f"–ó–æ–±—Ä–∞–∂–µ–Ω–Ω—è –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ: {filename}")
        else:
            print(f"–ù–µ –≤–¥–∞–ª–æ—Å—è –∑–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ {url}")
    except Exception as e:
        print(f"–ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—ñ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è {url}: {e}")

def get_images_from_post(post_url):
    """–ó–∞–≤–∞–Ω—Ç–∞–∂—É—î –≤—Å—ñ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è –∑ –ø–æ—Å—Ç—É"""
    try:
        res = requests.get(post_url)
        if res.status_code != 200:
            print(f"–ü–æ–º–∏–ª–∫–∞ –¥–æ—Å—Ç—É–ø—É –¥–æ –Ω–æ–≤–∏–Ω–∏ {post_url}")
            return []
        soup = BeautifulSoup(res.text, "html.parser")
        images = []
        for img_tag in soup.find_all("img"):
            img_url = img_tag.get("src")
            if img_url:
                full_img_url = urljoin(BASE_URL, img_url)
                images.append(full_img_url)
        return images
    except Exception as e:
        print(f"–ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –æ—Ç—Ä–∏–º–∞–Ω–Ω—ñ –∑–æ–±—Ä–∞–∂–µ–Ω—å –∑ {post_url}: {e}")
        return []

def download_all_images(all_news):
    print("üì• –ü–æ—á–∏–Ω–∞—î–º–æ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –∑–æ–±—Ä–∞–∂–µ–Ω—å...")
    downloaded = set()
    for post in all_news:
        post_url = post["url"]
        print(f"–ó–±–∏—Ä–∞—î–º–æ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è –∑: {post_url}")
        images = get_images_from_post(post_url)
        for img_url in images:
            if img_url not in downloaded:
                download_image(img_url)
                downloaded.add(img_url)
    print(f"‚úÖ –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ {len(downloaded)} —É–Ω—ñ–∫–∞–ª—å–Ω–∏—Ö –∑–æ–±—Ä–∞–∂–µ–Ω—å")

def fetch_all_news():
    all_news = []
    quan = 10
    print("–°—Ç–∞—Ä—Ç –µ–∫—Å–ø–æ—Ä—Ç—É –Ω–æ–≤–∏–Ω‚Ä¶")
    while True:
        print(f"–ó–∞–ø–∏—Ç {quan} –Ω–æ–≤–∏–Ω‚Ä¶")
        links = get_news_links(quan=quan)
        if not links:
            break
        new_news = []
        for url in links:
            if url not in [n["url"] for n in all_news]:
                details = get_news_details(url)
                if details:
                    new_news.append(details)
        if not new_news:
            break
        all_news.extend(new_news)
        print(f"–ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ –Ω–æ–≤–∏–Ω: {len(all_news)}")
        quan += 10
    export_to_json(all_news)
    download_all_images(all_news)

if __name__ == "__main__":
    fetch_all_news()
