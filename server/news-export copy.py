import os
import json
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin
from datetime import datetime

BASE_URL = "https://yevshan.com.ua"
NEWS_URL = BASE_URL + "/news"
IMG_FOLDER = "photo/news-images/"

os.makedirs(IMG_FOLDER, exist_ok=True)

new_news = []
failed_news = []

TARGET_MONTH = 1
TARGET_YEAR = 2021


def get_all_news():
    all_posts = []
    offset = 0
    stop = False

    while not stop:
        try:
            res = requests.post(
                urljoin(BASE_URL, "php/functions/loadNews.php"),
                data={"quan": offset}
            )
            res.raise_for_status()
            soup = BeautifulSoup(res.text, "html.parser")
            page_posts = []

            for post_div in soup.find_all("div", class_="content-post"):
                try:
                    post_id = post_div.get("data-id")
                    title_div = post_div.find("div", class_="content-post-title")
                    title = title_div.get_text(strip=True) if title_div else "Ð‘ÐµÐ· Ð½Ð°Ð·Ð²Ð¸"
                    date_div = post_div.find("div", class_="content-post-info-date")
                    updateDate = date_div.get_text(strip=True) if date_div else "01.01.1970"
                    dt = datetime.strptime(updateDate, "%d.%m.%Y")
                    year = dt.year
                    more_link = post_div.find("div", class_="content-post-continue")
                    url = urljoin(BASE_URL, more_link.find("a")["href"]) if more_link and more_link.find("a") else None

                   
                    if year < TARGET_YEAR:
                        stop = True
                        break
                    # âœ… ÑÐºÑ‰Ð¾ ÑÐ°Ð¼Ðµ Ð¿Ð¾Ñ‚Ñ€Ñ–Ð±Ð½Ð¸Ð¹ Ñ€Ñ–Ðº â€“ Ð´Ð¾Ð´Ð°Ñ”Ð¼Ð¾
                    else:
                        page_posts.append({
                            "id": post_id,
                            "title": title,
                            "updateDate": updateDate,
                            "year": year,
                            "url": url
                        })
                except Exception as e:
                    print(f"âŒ ÐŸÐ¾Ð¼Ð¸Ð»ÐºÐ° Ð¿Ñ€Ð¸ Ð¿Ð°Ñ€ÑÐ¸Ð½Ð³Ñƒ Ð¿Ð¾ÑÑ‚Ñƒ: {e}")

            if not page_posts and not stop:
                break  # Ð±Ñ–Ð»ÑŒÑˆÐµ Ð½Ð¾Ð²Ð¸Ð½ Ð½ÐµÐ¼Ð°

            all_posts.extend(page_posts)
            offset += 10
            print(f"ðŸ“¥ Offset {offset}, Ð·Ñ–Ð±Ñ€Ð°Ð½Ð¾ {len(all_posts)} Ð½Ð¾Ð²Ð¸Ð½ ({year})")

        except Exception as e:
            print(f"âŒ ÐŸÐ¾Ð¼Ð¸Ð»ÐºÐ° Ð¿Ñ€Ð¸ Ð¾Ñ‚Ñ€Ð¸Ð¼Ð°Ð½Ð½Ñ– Ð½Ð¾Ð²Ð¸Ð½: {e}")
            break

    print(f"âœ… Ð—Ð°Ð²Ð°Ð½Ñ‚Ð°Ð¶ÐµÐ½Ð¾ Ð½Ð¾Ð²Ð¸Ð½ Ð·Ð° {TARGET_YEAR}: {len(all_posts)}")
    return all_posts

def get_full_content(news_url):
    """ÐžÑ‚Ñ€Ð¸Ð¼ÑƒÑ”Ð¼Ð¾ ÐºÐ¾Ð½Ñ‚ÐµÐ½Ñ‚ Ð½Ð¾Ð²Ð¸Ð½Ð¸ Ð· Ñ—Ñ— ÑÑ‚Ð¾Ñ€Ñ–Ð½ÐºÐ¸"""
    try:
        res = requests.get(news_url)
        res.raise_for_status()
        soup = BeautifulSoup(res.text, "html.parser")

        content_div = soup.find_all("div", class_="news-text")
        photos = soup.find_all("div", class_="news-photo")

        parts = []
               
        for div in content_div:
            html = str(div)
            parts.append(html)

        for ph in photos:
            html = str(ph)
            html = html.replace('src="/photo/news-images/', 'src="/school-site/photo/news-images/')
            parts.append(html)

        return "\n".join(parts)
    except Exception as e:
        print(f"âŒ ÐŸÐ¾Ð¼Ð¸Ð»ÐºÐ° Ð¿Ñ€Ð¸ Ð·Ð°Ð²Ð°Ð½Ñ‚Ð°Ð¶ÐµÐ½Ð½Ñ– ÐºÐ¾Ð½Ñ‚ÐµÐ½Ñ‚Ñƒ {news_url}: {e}")
        return ""

def download_image(url, folder=IMG_FOLDER):
    """Ð—Ð°Ð²Ð°Ð½Ñ‚Ð°Ð¶ÑƒÑ”Ð¼Ð¾ Ð¾Ð´Ð½Ðµ Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð½Ñ"""
    try:
        if not os.path.exists(folder):
            os.makedirs(folder)
        filename = os.path.join(folder, url.split("/")[-1])
        response = requests.get(url, stream=True)
        if response.status_code == 200:
            with open(filename, "wb") as f:
                for chunk in response.iter_content(1024):
                    f.write(chunk)
            print(f"Ð—Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð½Ñ Ð·Ð°Ð²Ð°Ð½Ñ‚Ð°Ð¶ÐµÐ½Ð¾: {filename}")
        else:
            print(f"ÐÐµ Ð²Ð´Ð°Ð»Ð¾ÑÑ Ð·Ð°Ð²Ð°Ð½Ñ‚Ð°Ð¶Ð¸Ñ‚Ð¸ {url}")
    except Exception as e:
        print(f"ÐŸÐ¾Ð¼Ð¸Ð»ÐºÐ° Ð¿Ñ€Ð¸ Ð·Ð°Ð²Ð°Ð½Ñ‚Ð°Ð¶ÐµÐ½Ð½Ñ– Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð½Ñ {url}: {e}")


def get_images_from_post(post_url):
    """ÐŸÐ¾Ð²ÐµÑ€Ñ‚Ð°Ñ” ÑÐ¿Ð¸ÑÐ¾Ðº Ð²ÑÑ–Ñ… Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½ÑŒ Ñƒ Ð¿Ð¾ÑÑ‚Ñ–"""
    try:
        res = requests.get(post_url)
        if res.status_code != 200:
            print(f"ÐŸÐ¾Ð¼Ð¸Ð»ÐºÐ° Ð´Ð¾ÑÑ‚ÑƒÐ¿Ñƒ Ð´Ð¾ Ð½Ð¾Ð²Ð¸Ð½Ð¸ {post_url}")
            return []
        soup = BeautifulSoup(res.text, "html.parser")
        images = []
        for img_tag in soup.find_all("img"):
            img_url = img_tag.get("src")
            if img_url:
                full_img_url = urljoin(BASE_URL, img_url)
                images.append(full_img_url)
        return images
    except Exception as e:
        print(f"ÐŸÐ¾Ð¼Ð¸Ð»ÐºÐ° Ð¿Ñ€Ð¸ Ð¾Ñ‚Ñ€Ð¸Ð¼Ð°Ð½Ð½Ñ– Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½ÑŒ Ð· {post_url}: {e}")
        return []


def fetch_all_news():
    all_news = []
    quan = 10
    print("Ð¡Ñ‚Ð°Ñ€Ñ‚ ÐµÐºÑÐ¿Ð¾Ñ€Ñ‚Ñƒ Ð½Ð¾Ð²Ð¸Ð½â€¦")

    while True:
        print(f"Ð—Ð°Ð¿Ð¸Ñ‚ {quan} Ð½Ð¾Ð²Ð¸Ð½â€¦")
        posts = get_all_news()
        if not posts:
            break

        new_news = []
        for post in posts:
            if post["url"] and post["url"] not in [n["url"] for n in all_news]:
                content = get_full_content(post["url"])
                post["content"] = content
                new_news.append(post)

                images = get_images_from_post(post["url"])
                for img_url in images:
                    download_image(img_url)

        if not new_news:
            break

        all_news.extend(new_news)
        print(f"Ð—Ð°Ð²Ð°Ð½Ñ‚Ð°Ð¶ÐµÐ½Ð¾ Ð½Ð¾Ð²Ð¸Ð½: {len(all_news)}")
        quan += 10

    # Ð·Ð±ÐµÑ€ÐµÐ¶ÐµÐ½Ð½Ñ Ñƒ Ñ„Ð°Ð¹Ð»
    filename = "news_after_2025.json"
    with open(filename, "w", encoding="utf-8") as f:
        json.dump(all_news, f, ensure_ascii=False, indent=4)
    print(f"âœ… Ð—Ð±ÐµÑ€ÐµÐ¶ÐµÐ½Ð¾ {len(all_news)} Ð½Ð¾Ð²Ð¸Ð½ Ñƒ {filename}")

if __name__ == "__main__":
    fetch_all_news()
    